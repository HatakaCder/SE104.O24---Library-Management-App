{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":50160,"databundleVersionId":7921029,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/phtlngi/home-credit?scriptVersionId=181867109\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"%%writefile script.py\nimport sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\nROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n\n#Chuyển đổi dữ liệu trên dataframe\nclass Pipeline:\n\n    def set_table_dtypes(df):\n        for col in df.columns:\n            #chuyển các cột sau sang kiểu int64\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            #chuyển thành date\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n    \n    #tính toán sự khác biệt giữa cột đó và cột date_decision\n    #Chuyển đổi kết quả thành tổng số ngày\n    #Loại bỏ các cột date_decision và Month\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    #Loại bỏ các cột k nằm trong danh sách và thuộc chuỗi\n    def filter_cols(df):\n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n\n\nclass Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        return expr_max\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        return  expr_max\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        return  expr_max\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        return  expr_max \n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        return  expr_max\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs\n\ndef read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df\n\ndef feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base\n\ndef to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    return df_data, cat_cols\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2    \n    return df\n\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n\ndata_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n    ]\n}\n\ndf_train = feature_eng(**data_store)\ndel data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nnums=df_train.select_dtypes(exclude='category').columns\nfrom itertools import combinations, permutations\nnans_df = df_train[nums].isna()\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\ndef reduce_group(grps):\n    use = []\n    for g in grps:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = df_train[gg].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n        use.append(vx)\n    return use\n\ndef group_columns_by_correlation(matrix, threshold=0.8):\n    correlation_matrix = matrix.corr()\n    groups = []\n    remaining_cols = list(matrix.columns)\n    while remaining_cols:\n        col = remaining_cols.pop(0)\n        group = [col]\n        correlated_cols = [col]\n        for c in remaining_cols:\n            if correlation_matrix.loc[col, c] >= threshold:\n                group.append(c)\n                correlated_cols.append(c)\n        groups.append(group)\n        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n    \n    return groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n            use=reduce_group(grps)\n            uses=uses+use\n    else:\n        uses=uses+v\ndf_train=df_train[uses]\n\ndata_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n    ]\n}\n\ndf_test = feature_eng(**data_store)\ndel data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\ndf_test, cat_cols = to_pandas(df_test)\ndf_test = reduce_mem_usage(df_test)\ngc.collect()\n\ndf_train['target']=0\ndf_test['target']=1\n\ndf_train=pd.concat([df_train,df_test])\ndf_train=reduce_mem_usage(df_train)\n\ny = df_train[\"target\"]\ndf_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n\njoblib.dump((df_train,y,df_test),'data.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:11:14.588415Z","iopub.execute_input":"2024-05-26T16:11:14.588798Z","iopub.status.idle":"2024-05-26T16:11:14.60929Z","shell.execute_reply.started":"2024-05-26T16:11:14.58877Z","shell.execute_reply":"2024-05-26T16:11:14.608398Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Writing script.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python script.py","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:11:14.610849Z","iopub.execute_input":"2024-05-26T16:11:14.611125Z","iopub.status.idle":"2024-05-26T16:13:59.997758Z","shell.execute_reply.started":"2024-05-26T16:11:14.6111Z","shell.execute_reply":"2024-05-26T16:13:59.996584Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%writefile baseline.py\nimport sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nROOT = '/kaggle/input/home-credit-credit-risk-model-stability'\n\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier, Pool\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-26T16:13:59.999213Z","iopub.execute_input":"2024-05-26T16:13:59.999535Z","iopub.status.idle":"2024-05-26T16:14:00.006269Z","shell.execute_reply.started":"2024-05-26T16:13:59.999477Z","shell.execute_reply":"2024-05-26T16:14:00.005442Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\n# Set a seed for various non-deterministic processes for reproducibility\nimport random\ndef seed_it_all(seed=7):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\nSEED = 0\n\n# set the seed for this run\nseed_it_all(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.008645Z","iopub.execute_input":"2024-05-26T16:14:00.009195Z","iopub.status.idle":"2024-05-26T16:14:00.02024Z","shell.execute_reply.started":"2024-05-26T16:14:00.009162Z","shell.execute_reply":"2024-05-26T16:14:00.019407Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nclass Pipeline:\n\n    def set_table_dtypes(df):\n        for col in df.columns:\n            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Int64))\n            elif col in [\"date_decision\"]:\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n            elif col[-1] in (\"P\", \"A\"):\n                df = df.with_columns(pl.col(col).cast(pl.Float64))\n            elif col[-1] in (\"M\",):\n                df = df.with_columns(pl.col(col).cast(pl.String))\n            elif col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col).cast(pl.Date))\n        return df\n\n    def handle_dates(df):\n        for col in df.columns:\n            if col[-1] in (\"D\",):\n                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n        df = df.drop(\"date_decision\", \"MONTH\")\n        return df\n\n    def filter_cols(df):\n        for col in df.columns:\n            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n                isnull = df[col].is_null().mean()\n                if isnull > 0.7:\n                    df = df.drop(col)\n        \n        for col in df.columns:\n            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n                freq = df[col].n_unique()\n                if (freq == 1) | (freq > 200):\n                    df = df.drop(col)\n        \n        return df\n\n\nclass Aggregator:\n    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n    def num_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        \n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return expr_max +expr_last+expr_mean\n    \n    def date_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"D\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n        return  expr_max +expr_last+expr_mean\n    \n    def str_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        #expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n        return  expr_max +expr_last#+expr_count\n    \n    def other_expr(df):\n        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def count_expr(df):\n        cols = [col for col in df.columns if \"num_group\" in col]\n        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n        #expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n        #expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n        return  expr_max +expr_last\n    \n    def get_exprs(df):\n        exprs = Aggregator.num_expr(df) + \\\n                Aggregator.date_expr(df) + \\\n                Aggregator.str_expr(df) + \\\n                Aggregator.other_expr(df) + \\\n                Aggregator.count_expr(df)\n\n        return exprs\n\ndef read_file(path, depth=None):\n    df = pl.read_parquet(path)\n    df = df.pipe(Pipeline.set_table_dtypes)\n    if depth in [1,2]:\n        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n    return df\n\ndef read_files(regex_path, depth=None):\n    chunks = []\n    \n    for path in glob(str(regex_path)):\n        df = pl.read_parquet(path)\n        df = df.pipe(Pipeline.set_table_dtypes)\n        if depth in [1, 2]:\n            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n        chunks.append(df)\n    \n    df = pl.concat(chunks, how=\"vertical_relaxed\")\n    df = df.unique(subset=[\"case_id\"])\n    return df\n\ndef feature_eng(df_base, depth_0, depth_1, depth_2):\n    df_base = (\n        df_base\n        .with_columns(\n            month_decision = pl.col(\"date_decision\").dt.month(),\n            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n        )\n    )\n    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n    df_base = df_base.pipe(Pipeline.handle_dates)\n    return df_base\n\ndef to_pandas(df_data, cat_cols=None):\n    df_data = df_data.to_pandas()\n    if cat_cols is None:\n        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n    return df_data, cat_cols\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        if str(col_type)==\"category\":\n            continue\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            continue\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.021705Z","iopub.execute_input":"2024-05-26T16:14:00.021967Z","iopub.status.idle":"2024-05-26T16:14:00.035299Z","shell.execute_reply.started":"2024-05-26T16:14:00.021945Z","shell.execute_reply":"2024-05-26T16:14:00.034469Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n\nTRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\nTEST_DIR        = ROOT / \"parquet_files\" / \"test\"","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.036616Z","iopub.execute_input":"2024-05-26T16:14:00.036889Z","iopub.status.idle":"2024-05-26T16:14:00.04983Z","shell.execute_reply.started":"2024-05-26T16:14:00.036857Z","shell.execute_reply":"2024-05-26T16:14:00.048951Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ndata_store = {\n    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n    \"depth_0\": [\n        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n        read_files(TRAIN_DIR / \"train_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_applprev_2.parquet\", 2),\n        read_file(TRAIN_DIR / \"train_person_2.parquet\", 2)\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.050917Z","iopub.execute_input":"2024-05-26T16:14:00.051249Z","iopub.status.idle":"2024-05-26T16:14:00.059095Z","shell.execute_reply.started":"2024-05-26T16:14:00.051218Z","shell.execute_reply":"2024-05-26T16:14:00.058287Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ndf_train = feature_eng(**data_store)\nprint(\"train data shape:\\t\", df_train.shape)\ndel data_store\ngc.collect()\ndf_train = df_train.pipe(Pipeline.filter_cols)\ndf_train, cat_cols = to_pandas(df_train)\ndf_train = reduce_mem_usage(df_train)\nprint(\"train data shape:\\t\", df_train.shape)\nnums=df_train.select_dtypes(exclude='category').columns\nfrom itertools import combinations, permutations\n#df_train=df_train[nums]\nnans_df = df_train[nums].isna()\nnans_groups={}\nfor col in nums:\n    cur_group = nans_df[col].sum()\n    try:\n        nans_groups[cur_group].append(col)\n    except:\n        nans_groups[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\ndef reduce_group(grps):\n    use = []\n    for g in grps:\n        mx = 0; vx = g[0]\n        for gg in g:\n            n = df_train[gg].nunique()\n            if n>mx:\n                mx = n\n                vx = gg\n            #print(str(gg)+'-'+str(n),', ',end='')\n        use.append(vx)\n        #print()\n    print('Use these',use)\n    return use\n\ndef group_columns_by_correlation(matrix, threshold=0.8):\n    # 计算列之间的相关性\n    correlation_matrix = matrix.corr()\n\n    # 分组列\n    groups = []\n    remaining_cols = list(matrix.columns)\n    while remaining_cols:\n        col = remaining_cols.pop(0)\n        group = [col]\n        correlated_cols = [col]\n        for c in remaining_cols:\n            if correlation_matrix.loc[col, c] >= threshold:\n                group.append(c)\n                correlated_cols.append(c)\n        groups.append(group)\n        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n    \n    return groups\n\nuses=[]\nfor k,v in nans_groups.items():\n    if len(v)>1:\n            Vs = nans_groups[k]\n            #cross_features=list(combinations(Vs, 2))\n            #make_corr(Vs)\n            grps= group_columns_by_correlation(df_train[Vs], threshold=0.8)\n            use=reduce_group(grps)\n            uses=uses+use\n            #make_corr(use)\n    else:\n        uses=uses+v\n    print('####### NAN count =',k)\nprint(uses)\nprint(len(uses))\nuses=uses+list(df_train.select_dtypes(include='category').columns)\nprint(len(uses))\ndf_train=df_train[uses]","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.060403Z","iopub.execute_input":"2024-05-26T16:14:00.060701Z","iopub.status.idle":"2024-05-26T16:14:00.072837Z","shell.execute_reply.started":"2024-05-26T16:14:00.060678Z","shell.execute_reply":"2024-05-26T16:14:00.071994Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nsample = pd.read_csv(\"/kaggle/input/home-credit-credit-risk-model-stability/sample_submission.csv\")\ndevice='gpu'\nn_est=6000\nDRY_RUN = True if sample.shape[0] == 10 else False   \nif DRY_RUN:\n    device='cpu'\n    df_train = df_train.iloc[:50000]\n    n_est=600\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.07586Z","iopub.execute_input":"2024-05-26T16:14:00.076106Z","iopub.status.idle":"2024-05-26T16:14:00.087657Z","shell.execute_reply.started":"2024-05-26T16:14:00.076085Z","shell.execute_reply":"2024-05-26T16:14:00.086801Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ndata_store = {\n    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n    \"depth_0\": [\n        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n    ],\n    \"depth_1\": [\n        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n    ],\n    \"depth_2\": [\n        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n    ]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.088797Z","iopub.execute_input":"2024-05-26T16:14:00.089099Z","iopub.status.idle":"2024-05-26T16:14:00.09936Z","shell.execute_reply.started":"2024-05-26T16:14:00.08906Z","shell.execute_reply":"2024-05-26T16:14:00.098521Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ndf_test = feature_eng(**data_store)\nprint(\"test data shape:\\t\", df_test.shape)\ndel data_store\ngc.collect()\ndf_test = df_test.select([col for col in df_train.columns if col != \"target\"])\nprint(\"train data shape:\\t\", df_train.shape)\nprint(\"test data shape:\\t\", df_test.shape)\n\ndf_test, cat_cols = to_pandas(df_test, cat_cols)\ndf_test = reduce_mem_usage(df_test)\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.100653Z","iopub.execute_input":"2024-05-26T16:14:00.101448Z","iopub.status.idle":"2024-05-26T16:14:00.113896Z","shell.execute_reply.started":"2024-05-26T16:14:00.101423Z","shell.execute_reply":"2024-05-26T16:14:00.113131Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Feature Selection","metadata":{}},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ny = df_train[\"target\"]\nweeks = df_train[\"WEEK_NUM\"]\ndf_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\ncv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=SEED)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.114883Z","iopub.execute_input":"2024-05-26T16:14:00.115198Z","iopub.status.idle":"2024-05-26T16:14:00.122839Z","shell.execute_reply.started":"2024-05-26T16:14:00.115175Z","shell.execute_reply":"2024-05-26T16:14:00.121813Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ndf_train[cat_cols] = df_train[cat_cols].astype(str)\ndf_test[cat_cols] = df_test[cat_cols].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.123851Z","iopub.execute_input":"2024-05-26T16:14:00.124091Z","iopub.status.idle":"2024-05-26T16:14:00.131097Z","shell.execute_reply.started":"2024-05-26T16:14:00.12407Z","shell.execute_reply":"2024-05-26T16:14:00.130195Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nparams_lgb = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2500,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": SEED,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n}","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.132145Z","iopub.execute_input":"2024-05-26T16:14:00.132417Z","iopub.status.idle":"2024-05-26T16:14:00.144901Z","shell.execute_reply.started":"2024-05-26T16:14:00.132389Z","shell.execute_reply":"2024-05-26T16:14:00.14414Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nparams_lgb2 = {\n    \"boosting_type\": \"goss\",\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"max_depth\": 10,  \n    \"learning_rate\": 0.05,\n    \"n_estimators\": 2500,  \n    \"colsample_bytree\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"verbose\": -1,\n    \"random_state\": SEED,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 10,\n    \"extra_trees\":True,\n    'num_leaves':64,\n    \"device\": device, \n}","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.146198Z","iopub.execute_input":"2024-05-26T16:14:00.146758Z","iopub.status.idle":"2024-05-26T16:14:00.155106Z","shell.execute_reply.started":"2024-05-26T16:14:00.146728Z","shell.execute_reply":"2024-05-26T16:14:00.154243Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nfitted_models_cb = []\nfitted_models_lgb = []\nfitted_models_lgb2 = []\nfitted_models_eclf = []\ncv_scores_cb = []\ncv_scores_lgb = []\ncv_scores_lgb2 = []\ncv_scores_eclf = []\n\nfor idx_train, idx_valid in cv.split(df_train, y, groups=weeks):#\n    X_train, y_train = df_train.iloc[idx_train], y.iloc[idx_train]# \n    X_valid, y_valid = df_train.iloc[idx_valid], y.iloc[idx_valid]\n    train_pool = Pool(X_train, y_train, cat_features=cat_cols)\n    val_pool = Pool(X_valid, y_valid, cat_features=cat_cols)\n    clf_cb = CatBoostClassifier(\n        eval_metric='AUC',\n        task_type='GPU',\n        learning_rate=0.05,\n        iterations=n_est)\n    random_seed=SEED\n    clf_cb.fit(train_pool, eval_set=val_pool,verbose=300)\n    fitted_models_cb.append(clf_cb)\n    y_pred_valid = clf_cb.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_cb.append(auc_score)\n    \n    \n    X_train[cat_cols] = X_train[cat_cols].astype(\"category\")\n    X_valid[cat_cols] = X_valid[cat_cols].astype(\"category\")\n    \n    clf_lgb = LGBMClassifier(**params_lgb)\n    clf_lgb.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n    \n    fitted_models_lgb.append(clf_lgb)\n    y_pred_valid = clf_lgb.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_lgb.append(auc_score)\n    \n    clf_lgb2 = LGBMClassifier(**params_lgb2)\n    clf_lgb2.fit(\n        X_train, y_train,\n        eval_set = [(X_valid, y_valid)],\n        callbacks = [lgb.log_evaluation(200), lgb.early_stopping(60)] )\n    \n    fitted_models_lgb2.append(clf_lgb2)\n    y_pred_valid = clf_lgb2.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_lgb2.append(auc_score)\n \n    eclf = VotingClassifier(\n     estimators=[('lgb', clf_lgb), ('lgb2', clf_lgb2)],\n     voting='soft', weights=[1, 1])   \n    eclf = eclf.fit(X_train, y_train)\n    fitted_models_eclf.append(eclf)\n    y_pred_valid = eclf.predict_proba(X_valid)[:,1]\n    auc_score = roc_auc_score(y_valid, y_pred_valid)\n    cv_scores_eclf.append(auc_score)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.156162Z","iopub.execute_input":"2024-05-26T16:14:00.156402Z","iopub.status.idle":"2024-05-26T16:14:00.16625Z","shell.execute_reply.started":"2024-05-26T16:14:00.15638Z","shell.execute_reply":"2024-05-26T16:14:00.165379Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nprint(\"CatBoost\")   \nprint(\"CV AUC scores: \", cv_scores_cb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_cb))\nprint(\"LightGBM\")\nprint(\"CV AUC scores: \", cv_scores_lgb)\nprint(\"Maximum CV AUC score: \", max(cv_scores_lgb))\nprint(\"LightGBM_goss\")\nprint(\"CV AUC scores: \", cv_scores_lgb2)\nprint(\"Maximum CV AUC score: \", max(cv_scores_lgb2))\nprint(\"Ensemble of LGBM and LGBM_goss\")\nprint(\"CV AUC scores: \", cv_scores_eclf)\nprint(\"Maximum CV AUC score: \", max(cv_scores_eclf))","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.167257Z","iopub.execute_input":"2024-05-26T16:14:00.167526Z","iopub.status.idle":"2024-05-26T16:14:00.179332Z","shell.execute_reply.started":"2024-05-26T16:14:00.167497Z","shell.execute_reply":"2024-05-26T16:14:00.178523Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a baseline.py\n\nclass VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        \n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n        \n        X[cat_cols] = X[cat_cols].astype(\"category\")\n        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[5:]]\n        \n        return np.mean(y_preds, axis=0)\n\nmodel = VotingModel(fitted_models_cb+fitted_models_eclf)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.180288Z","iopub.execute_input":"2024-05-26T16:14:00.180567Z","iopub.status.idle":"2024-05-26T16:14:00.187871Z","shell.execute_reply.started":"2024-05-26T16:14:00.180544Z","shell.execute_reply":"2024-05-26T16:14:00.187072Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"%%writefile -a baseline.py\n\ndf_test = df_test.drop(columns=[\"WEEK_NUM\"])\ndf_test = df_test.set_index(\"case_id\")\n\n\ny_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\ndf_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\ndf_subm = df_subm.set_index(\"case_id\")\n\ndf_subm[\"score\"] = y_pred\ndf_subm.to_csv(\"sub.csv\")\ndf_subm","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.188956Z","iopub.execute_input":"2024-05-26T16:14:00.189218Z","iopub.status.idle":"2024-05-26T16:14:00.200775Z","shell.execute_reply.started":"2024-05-26T16:14:00.189196Z","shell.execute_reply":"2024-05-26T16:14:00.199962Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Appending to baseline.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!python baseline.py","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:14:00.201724Z","iopub.execute_input":"2024-05-26T16:14:00.20199Z","iopub.status.idle":"2024-05-26T16:41:35.580599Z","shell.execute_reply.started":"2024-05-26T16:14:00.201959Z","shell.execute_reply":"2024-05-26T16:41:35.579333Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"train data shape:\t (1526659, 861)\nMemory usage of dataframe is 4322.75 MB\nMemory usage after optimization is: 1528.81 MB\nDecreased by 64.6%\ntrain data shape:\t (1526659, 472)\nUse these ['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9']\n####### NAN count = 0\n####### NAN count = 918788\nUse these ['dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'mean_refreshdate_3813885D']\n####### NAN count = 140968\nUse these ['pmtscount_423L', 'pmtssum_45A']\n####### NAN count = 954021\n####### NAN count = 806659\n####### NAN count = 866332\n####### NAN count = 418178\nUse these ['amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L']\n####### NAN count = 561124\nUse these ['annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A']\n####### NAN count = 4\nUse these ['mindbddpdlast24m_3658935P']\n####### NAN count = 613202\n####### NAN count = 948244\nUse these ['mindbdtollast24m_4525191P']\n####### NAN count = 972827\n####### NAN count = 467175\nUse these ['avginstallast24m_3658937A', 'maxinstallast24m_3658928A']\n####### NAN count = 624875\n####### NAN count = 757006\n####### NAN count = 841181\n####### NAN count = 1026987\n####### NAN count = 455190\n####### NAN count = 460822\nUse these ['commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P']\n####### NAN count = 343375\n####### NAN count = 833735\n####### NAN count = 887659\nUse these ['daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L']\n####### NAN count = 452594\n####### NAN count = 977119\nUse these ['eir_270L']\n####### NAN count = 190833\n####### NAN count = 859214\n####### NAN count = 482103\n####### NAN count = 453587\nUse these ['lastapplicationdate_877D', 'mean_creationdate_885D', 'max_num_group1', 'last_num_group1', 'max_num_group2_14', 'last_num_group2_14']\n####### NAN count = 305137\nUse these ['lastapprcredamount_781A', 'lastapprdate_640D']\n####### NAN count = 442041\n####### NAN count = 977975\nUse these ['lastrejectcredamount_222A', 'lastrejectdate_50D']\n####### NAN count = 769046\n####### NAN count = 511255\nUse these ['mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P']\n####### NAN count = 306019\n####### NAN count = 960953\n####### NAN count = 705504\n####### NAN count = 876276\n####### NAN count = 826000\n####### NAN count = 829402\n####### NAN count = 1032856\n####### NAN count = 766958\nUse these ['numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L']\n####### NAN count = 452593\n####### NAN count = 455081\nUse these ['numinstlsallpaid_934L']\n####### NAN count = 445669\nUse these ['numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L']\n####### NAN count = 456495\nUse these ['numinstpaid_4499208L']\n####### NAN count = 847191\n####### NAN count = 446983\nUse these ['numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A']\n####### NAN count = 840646\n####### NAN count = 669186\n####### NAN count = 455612\nUse these ['pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L']\n####### NAN count = 458738\n####### NAN count = 461362\n####### NAN count = 459827\n####### NAN count = 460079\n####### NAN count = 44954\n####### NAN count = 78526\n####### NAN count = 131888\n####### NAN count = 181122\n####### NAN count = 223240\n####### NAN count = 445320\n####### NAN count = 3\nUse these ['mean_actualdpd_943P']\n####### NAN count = 305154\nUse these ['max_annuity_853A', 'mean_annuity_853A']\n####### NAN count = 308739\nUse these ['max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A']\n####### NAN count = 307441\nUse these ['max_currdebt_94A', 'mean_currdebt_94A']\n####### NAN count = 419006\nUse these ['max_mainoccupationinc_437A', 'mean_mainoccupationinc_437A']\n####### NAN count = 306361\nUse these ['mean_maxdpdtolerance_577P']\n####### NAN count = 450969\nUse these ['max_outstandingdebt_522A', 'mean_outstandingdebt_522A']\n####### NAN count = 420383\n####### NAN count = 307551\n####### NAN count = 477657\n####### NAN count = 433335\nUse these ['last_credamount_590A', 'last_downpmt_134A']\n####### NAN count = 438219\n####### NAN count = 824731\n####### NAN count = 312491\n####### NAN count = 899665\n####### NAN count = 827764\nUse these ['max_approvaldate_319D', 'mean_approvaldate_319D']\n####### NAN count = 442999\nUse these ['max_dateactivated_425D', 'mean_dateactivated_425D']\n####### NAN count = 454678\nUse these ['max_dtlastpmt_581D', 'mean_dtlastpmt_581D']\n####### NAN count = 703840\nUse these ['max_dtlastpmtallstes_3545839D', 'mean_dtlastpmtallstes_3545839D']\n####### NAN count = 548987\nUse these ['max_employedfrom_700D']\n####### NAN count = 559169\nUse these ['max_firstnonzeroinstldate_307D', 'mean_firstnonzeroinstldate_307D']\n####### NAN count = 334873\n####### NAN count = 891021\n####### NAN count = 305203\n####### NAN count = 920818\n####### NAN count = 1016761\n####### NAN count = 1050001\n####### NAN count = 485683\n####### NAN count = 961606\n####### NAN count = 552766\nUse these ['max_pmtnum_8L']\n####### NAN count = 321446\nUse these ['last_pmtnum_8L']\n####### NAN count = 482174\nUse these ['max_pmtamount_36A', 'last_pmtamount_36A', 'max_processingdate_168D', 'last_processingdate_168D', 'max_num_group1_5']\n####### NAN count = 1044394\nUse these ['mean_credlmt_230A']\n####### NAN count = 1036944\nUse these ['mean_credlmt_935A']\n####### NAN count = 603001\nUse these ['mean_pmts_dpd_1073P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T']\n####### NAN count = 263166\nUse these ['max_pmts_dpd_303P', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P']\n####### NAN count = 514070\nUse these ['mean_instlamount_768A']\n####### NAN count = 606920\nUse these ['mean_monthlyinstlamount_332A']\n####### NAN count = 263233\nUse these ['max_monthlyinstlamount_674A', 'mean_monthlyinstlamount_674A']\n####### NAN count = 517511\nUse these ['mean_outstandingamount_354A']\n####### NAN count = 545885\nUse these ['mean_outstandingamount_362A']\n####### NAN count = 636453\nUse these ['mean_overdueamount_31A']\n####### NAN count = 512650\nUse these ['mean_overdueamount_659A', 'max_numberofoverdueinstls_725L']\n####### NAN count = 263171\nUse these ['mean_overdueamountmax2_14A', 'mean_totaloutstanddebtvalue_39A', 'mean_dateofcredend_289D', 'mean_dateofcredstart_739D', 'max_lastupdate_1112D', 'mean_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T']\n####### NAN count = 262653\nUse these ['mean_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'mean_dateofcredend_353D', 'max_numberofoverdueinstlmax_1151L']\n####### NAN count = 512590\nUse these ['mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A']\n####### NAN count = 513987\nUse these ['max_residualamount_488A']\n####### NAN count = 1039597\nUse these ['mean_residualamount_856A']\n####### NAN count = 606900\nUse these ['max_totalamount_6A', 'mean_totalamount_6A']\n####### NAN count = 545855\nUse these ['mean_totalamount_996A']\n####### NAN count = 636448\nUse these ['mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L']\n####### NAN count = 297072\nUse these ['max_dateofrealrepmt_138D', 'mean_dateofrealrepmt_138D']\n####### NAN count = 512961\nUse these ['max_lastupdate_388D', 'mean_lastupdate_388D']\n####### NAN count = 512591\nUse these ['max_numberofoverdueinstlmaxdat_148D']\n####### NAN count = 802351\nUse these ['mean_numberofoverdueinstlmaxdat_641D']\n####### NAN count = 1012361\nUse these ['mean_overdueamountmax2date_1002D']\n####### NAN count = 806653\nUse these ['max_overdueamountmax2date_1142D']\n####### NAN count = 1007594\n####### NAN count = 553734\n####### NAN count = 822517\n####### NAN count = 745109\n####### NAN count = 545898\n####### NAN count = 636545\n####### NAN count = 545895\n####### NAN count = 636544\n####### NAN count = 512657\n####### NAN count = 561307\n####### NAN count = 649082\nUse these ['last_num_group1_6']\n####### NAN count = 140386\nUse these ['last_mainoccupationinc_384A', 'last_birth_259D']\n####### NAN count = 750301\nUse these ['max_empl_employedfrom_271D']\n####### NAN count = 959958\nUse these ['last_personindex_1023L']\n####### NAN count = 587206\n####### NAN count = 772\n####### NAN count = 262659\n####### NAN count = 512884\nUse these ['max_pmts_month_706T', 'max_pmts_year_507T']\n####### NAN count = 512598\nUse these ['last_pmts_month_158T', 'last_pmts_year_1139T']\n####### NAN count = 994041\nUse these ['last_pmts_month_706T', 'last_pmts_year_507T']\n####### NAN count = 634357\nUse these ['max_num_group1_13', 'max_num_group2_13', 'last_num_group2_13']\n####### NAN count = 141371\nUse these ['max_num_group1_15', 'max_num_group2_15']\n####### NAN count = 91554\n['case_id', 'WEEK_NUM', 'target', 'month_decision', 'weekday_decision', 'credamount_770A', 'applicationcnt_361L', 'applications30d_658L', 'applicationscnt_1086L', 'applicationscnt_464L', 'applicationscnt_867L', 'clientscnt_1022L', 'clientscnt_100L', 'clientscnt_1071L', 'clientscnt_1130L', 'clientscnt_157L', 'clientscnt_257L', 'clientscnt_304L', 'clientscnt_360L', 'clientscnt_493L', 'clientscnt_533L', 'clientscnt_887L', 'clientscnt_946L', 'deferredmnthsnum_166L', 'disbursedcredamount_1113A', 'downpmt_116A', 'homephncnt_628L', 'isbidproduct_1095L', 'mobilephncnt_593L', 'numactivecreds_622L', 'numactivecredschannel_414L', 'numactiverelcontr_750L', 'numcontrs3months_479L', 'numnotactivated_1143L', 'numpmtchanneldd_318L', 'numrejects9m_859L', 'sellerplacecnt_915L', 'max_mainoccupationinc_384A', 'max_birth_259D', 'max_num_group1_9', 'birthdate_574D', 'dateofbirth_337D', 'days180_256L', 'days30_165L', 'days360_512L', 'firstquarter_103L', 'fourthquarter_440L', 'secondquarter_766L', 'thirdquarter_1082L', 'max_debtoutstand_525A', 'max_debtoverdue_47A', 'max_refreshdate_3813885D', 'mean_refreshdate_3813885D', 'pmtscount_423L', 'pmtssum_45A', 'responsedate_1012D', 'responsedate_4527233D', 'actualdpdtolerance_344P', 'amtinstpaidbefduel24m_4187115A', 'numinstlswithdpd5_4187116L', 'annuitynextmonth_57A', 'currdebt_22A', 'currdebtcredtyperange_828A', 'numinstls_657L', 'totalsettled_863A', 'mindbddpdlast24m_3658935P', 'avgdbddpdlast3m_4187120P', 'mindbdtollast24m_4525191P', 'avgdpdtolclosure24_3658938P', 'avginstallast24m_3658937A', 'maxinstallast24m_3658928A', 'avgmaxdpdlast9m_3716943P', 'avgoutstandbalancel6m_4187114A', 'avgpmtlast12m_4525200A', 'cntincpaycont9m_3716944L', 'cntpmts24_3658933L', 'commnoinclast6m_3546845L', 'maxdpdfrom6mto36m_3546853P', 'datefirstoffer_1144D', 'datelastunpaid_3546854D', 'daysoverduetolerancedd_3976961L', 'numinsttopaygr_769L', 'dtlastpmtallstes_4499206D', 'eir_270L', 'firstclxcampaign_1125D', 'firstdatedue_489D', 'lastactivateddate_801D', 'lastapplicationdate_877D', 'mean_creationdate_885D', 'max_num_group1', 'last_num_group1', 'max_num_group2_14', 'last_num_group2_14', 'lastapprcredamount_781A', 'lastapprdate_640D', 'lastdelinqdate_224D', 'lastrejectcredamount_222A', 'lastrejectdate_50D', 'maininc_215A', 'mastercontrelectronic_519L', 'mastercontrexist_109L', 'maxannuity_159A', 'maxdebt4_972A', 'maxdpdlast24m_143P', 'maxdpdlast3m_392P', 'maxdpdtolerance_374P', 'maxdbddpdlast1m_3658939P', 'maxdbddpdtollast12m_3658940P', 'maxdbddpdtollast6m_4187119P', 'maxdpdinstldate_3546855D', 'maxdpdinstlnum_3546846P', 'maxlnamtstart6m_4525199A', 'maxoutstandbalancel12m_4187113A', 'numinstpaidearly_338L', 'numinstpaidearly5d_1087L', 'numinstpaidlate1d_3546852L', 'numincomingpmts_3546848L', 'numinstlsallpaid_934L', 'numinstlswithdpd10_728L', 'numinstlswithoutdpd_562L', 'numinstpaid_4499208L', 'numinstpaidearly3d_3546850L', 'numinstregularpaidest_4493210L', 'numinstpaidearly5dest_4493211L', 'sumoutstandtotalest_4493215A', 'numinstpaidlastcontr_4325080L', 'numinstregularpaid_973L', 'pctinstlsallpaidearl3d_427L', 'pctinstlsallpaidlate1d_3546856L', 'pctinstlsallpaidlat10d_839L', 'pctinstlsallpaidlate4d_3546849L', 'pctinstlsallpaidlate6d_3546844L', 'pmtnum_254L', 'posfpd10lastmonth_333P', 'posfpd30lastmonth_3976960P', 'posfstqpd30lastmonth_3976962P', 'price_1097A', 'sumoutstandtotal_3546847A', 'totaldebt_9A', 'mean_actualdpd_943P', 'max_annuity_853A', 'mean_annuity_853A', 'max_credacc_credlmt_575A', 'max_credamount_590A', 'max_downpmt_134A', 'mean_credacc_credlmt_575A', 'mean_credamount_590A', 'mean_downpmt_134A', 'max_currdebt_94A', 'mean_currdebt_94A', 'max_mainoccupationinc_437A', 'mean_mainoccupationinc_437A', 'mean_maxdpdtolerance_577P', 'max_outstandingdebt_522A', 'mean_outstandingdebt_522A', 'last_actualdpd_943P', 'last_annuity_853A', 'last_credacc_credlmt_575A', 'last_credamount_590A', 'last_downpmt_134A', 'last_currdebt_94A', 'last_mainoccupationinc_437A', 'last_maxdpdtolerance_577P', 'last_outstandingdebt_522A', 'max_approvaldate_319D', 'mean_approvaldate_319D', 'max_dateactivated_425D', 'mean_dateactivated_425D', 'max_dtlastpmt_581D', 'mean_dtlastpmt_581D', 'max_dtlastpmtallstes_3545839D', 'mean_dtlastpmtallstes_3545839D', 'max_employedfrom_700D', 'max_firstnonzeroinstldate_307D', 'mean_firstnonzeroinstldate_307D', 'last_approvaldate_319D', 'last_creationdate_885D', 'last_dateactivated_425D', 'last_dtlastpmtallstes_3545839D', 'last_employedfrom_700D', 'last_firstnonzeroinstldate_307D', 'max_byoccupationinc_3656910L', 'max_childnum_21L', 'max_pmtnum_8L', 'last_pmtnum_8L', 'max_pmtamount_36A', 'last_pmtamount_36A', 'max_processingdate_168D', 'last_processingdate_168D', 'max_num_group1_5', 'mean_credlmt_230A', 'mean_credlmt_935A', 'mean_pmts_dpd_1073P', 'max_dpdmaxdatemonth_89T', 'max_dpdmaxdateyear_596T', 'max_pmts_dpd_303P', 'mean_dpdmax_757P', 'max_dpdmaxdatemonth_442T', 'max_dpdmaxdateyear_896T', 'mean_pmts_dpd_303P', 'mean_instlamount_768A', 'mean_monthlyinstlamount_332A', 'max_monthlyinstlamount_674A', 'mean_monthlyinstlamount_674A', 'mean_outstandingamount_354A', 'mean_outstandingamount_362A', 'mean_overdueamount_31A', 'mean_overdueamount_659A', 'max_numberofoverdueinstls_725L', 'mean_overdueamountmax2_14A', 'mean_totaloutstanddebtvalue_39A', 'mean_dateofcredend_289D', 'mean_dateofcredstart_739D', 'max_lastupdate_1112D', 'mean_lastupdate_1112D', 'max_numberofcontrsvalue_258L', 'max_numberofoverdueinstlmax_1039L', 'max_overdueamountmaxdatemonth_365T', 'max_overdueamountmaxdateyear_2T', 'mean_pmts_overdue_1140A', 'max_pmts_month_158T', 'max_pmts_year_1139T', 'mean_overdueamountmax2_398A', 'max_dateofcredend_353D', 'max_dateofcredstart_181D', 'mean_dateofcredend_353D', 'max_numberofoverdueinstlmax_1151L', 'mean_overdueamountmax_35A', 'max_overdueamountmaxdatemonth_284T', 'max_overdueamountmaxdateyear_994T', 'mean_pmts_overdue_1152A', 'max_residualamount_488A', 'mean_residualamount_856A', 'max_totalamount_6A', 'mean_totalamount_6A', 'mean_totalamount_996A', 'mean_totaldebtoverduevalue_718A', 'mean_totaloutstanddebtvalue_668A', 'max_numberofcontrsvalue_358L', 'max_dateofrealrepmt_138D', 'mean_dateofrealrepmt_138D', 'max_lastupdate_388D', 'mean_lastupdate_388D', 'max_numberofoverdueinstlmaxdat_148D', 'mean_numberofoverdueinstlmaxdat_641D', 'mean_overdueamountmax2date_1002D', 'max_overdueamountmax2date_1142D', 'last_refreshdate_3813885D', 'max_nominalrate_281L', 'max_nominalrate_498L', 'max_numberofinstls_229L', 'max_numberofinstls_320L', 'max_numberofoutstandinstls_520L', 'max_numberofoutstandinstls_59L', 'max_numberofoverdueinstls_834L', 'max_periodicityofpmts_1102L', 'max_periodicityofpmts_837L', 'last_num_group1_6', 'last_mainoccupationinc_384A', 'last_birth_259D', 'max_empl_employedfrom_271D', 'last_personindex_1023L', 'last_persontype_1072L', 'max_collater_valueofguarantee_1124L', 'max_collater_valueofguarantee_876L', 'max_pmts_month_706T', 'max_pmts_year_507T', 'last_pmts_month_158T', 'last_pmts_year_1139T', 'last_pmts_month_706T', 'last_pmts_year_507T', 'max_num_group1_13', 'max_num_group2_13', 'last_num_group2_13', 'max_num_group1_15', 'max_num_group2_15']\n276\n389\ntest data shape:\t (10, 860)\ntrain data shape:\t (50000, 389)\ntest data shape:\t (10, 388)\nMemory usage of dataframe is 0.04 MB\nMemory usage after optimization is: 0.02 MB\nDecreased by 40.3%\nDefault metric period is 5 because AUC is/are not implemented for GPU\n0:\ttest: 0.5792207\tbest: 0.5792207 (0)\ttotal: 421ms\tremaining: 4m 12s\n300:\ttest: 0.7541151\tbest: 0.7546397 (280)\ttotal: 27.7s\tremaining: 27.6s\n599:\ttest: 0.7560412\tbest: 0.7574959 (380)\ttotal: 55.4s\tremaining: 0us\nbestTest = 0.7574959397\nbestIteration = 380\nShrink model to first 381 iterations.\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[86]\tvalid_0's auc: 0.765434\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.757262\nEarly stopping, best iteration is:\n[143]\tvalid_0's auc: 0.7619\nDefault metric period is 5 because AUC is/are not implemented for GPU\n0:\ttest: 0.5796317\tbest: 0.5796317 (0)\ttotal: 135ms\tremaining: 1m 20s\n300:\ttest: 0.7438381\tbest: 0.7441371 (295)\ttotal: 27.8s\tremaining: 27.6s\n599:\ttest: 0.7494330\tbest: 0.7494330 (599)\ttotal: 55.1s\tremaining: 0us\nbestTest = 0.7494330108\nbestIteration = 599\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[111]\tvalid_0's auc: 0.754794\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[80]\tvalid_0's auc: 0.754695\nDefault metric period is 5 because AUC is/are not implemented for GPU\n0:\ttest: 0.5679812\tbest: 0.5679812 (0)\ttotal: 184ms\tremaining: 1m 50s\n300:\ttest: 0.7204567\tbest: 0.7204567 (300)\ttotal: 27.7s\tremaining: 27.5s\n599:\ttest: 0.7211076\tbest: 0.7217967 (545)\ttotal: 55.3s\tremaining: 0us\nbestTest = 0.7217966914\nbestIteration = 545\nShrink model to first 546 iterations.\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[95]\tvalid_0's auc: 0.729991\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[92]\tvalid_0's auc: 0.730187\nDefault metric period is 5 because AUC is/are not implemented for GPU\n0:\ttest: 0.6325521\tbest: 0.6325521 (0)\ttotal: 141ms\tremaining: 1m 24s\n300:\ttest: 0.7567488\tbest: 0.7567488 (300)\ttotal: 27.4s\tremaining: 27.2s\n599:\ttest: 0.7625934\tbest: 0.7625934 (599)\ttotal: 54.9s\tremaining: 0us\nbestTest = 0.7625933886\nbestIteration = 599\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[73]\tvalid_0's auc: 0.764708\nTraining until validation scores don't improve for 60 rounds\nEarly stopping, best iteration is:\n[85]\tvalid_0's auc: 0.766373\nDefault metric period is 5 because AUC is/are not implemented for GPU\n0:\ttest: 0.5882546\tbest: 0.5882546 (0)\ttotal: 136ms\tremaining: 1m 21s\n300:\ttest: 0.7128673\tbest: 0.7128673 (300)\ttotal: 27.2s\tremaining: 27.1s\n599:\ttest: 0.7200049\tbest: 0.7201002 (595)\ttotal: 54.3s\tremaining: 0us\nbestTest = 0.7201001942\nbestIteration = 595\nShrink model to first 596 iterations.\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.723181\nEarly stopping, best iteration is:\n[192]\tvalid_0's auc: 0.723802\nTraining until validation scores don't improve for 60 rounds\n[200]\tvalid_0's auc: 0.720869\nEarly stopping, best iteration is:\n[157]\tvalid_0's auc: 0.722364\nCatBoost\nCV AUC scores:  [0.7574952064152896, 0.7494331991423062, 0.7217970257898471, 0.7625935209294905, 0.7201001944367287]\nMaximum CV AUC score:  0.7625935209294905\nLightGBM\nCV AUC scores:  [0.7654337093743829, 0.7547939526010682, 0.7299913448565538, 0.7647079659417527, 0.7238016348557597]\nMaximum CV AUC score:  0.7654337093743829\nLightGBM_goss\nCV AUC scores:  [0.7619000647629788, 0.7546948380468184, 0.7301873257630201, 0.7663727534021745, 0.722364191897147]\nMaximum CV AUC score:  0.7663727534021745\nEnsemble of LGBM and LGBM_goss\nCV AUC scores:  [0.7092492267368219, 0.6865876841262835, 0.6753661954639476, 0.7253289962875537, 0.6843145113289155]\nMaximum CV AUC score:  0.7253289962875537\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nfrom pathlib import Path\nimport subprocess\nimport os\nimport gc\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport joblib\n","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:41:35.582001Z","iopub.execute_input":"2024-05-26T16:41:35.582319Z","iopub.status.idle":"2024-05-26T16:41:39.443323Z","shell.execute_reply.started":"2024-05-26T16:41:35.582283Z","shell.execute_reply":"2024-05-26T16:41:39.442532Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df_train,y,df_test=joblib.load('/kaggle/working/data.pkl')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:41:39.444451Z","iopub.execute_input":"2024-05-26T16:41:39.444826Z","iopub.status.idle":"2024-05-26T16:41:40.618382Z","shell.execute_reply.started":"2024-05-26T16:41:39.444793Z","shell.execute_reply":"2024-05-26T16:41:40.617372Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"fitted_models_lgb=[]\nmodel = lgb.LGBMClassifier()\nmodel.fit(df_train,y)\nfitted_models_lgb.append(model)  ","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:41:40.619639Z","iopub.execute_input":"2024-05-26T16:41:40.61995Z","iopub.status.idle":"2024-05-26T16:43:22.022246Z","shell.execute_reply.started":"2024-05-26T16:41:40.619923Z","shell.execute_reply":"2024-05-26T16:43:22.021217Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 10, number of negative: 1526659\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.084338 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 46717\n[LightGBM] [Info] Number of data points in the train set: 1526669, number of used features: 308\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000007 -> initscore=-11.936007\n[LightGBM] [Info] Start training from score -11.936007\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"}]},{"cell_type":"code","source":"class VotingModel(BaseEstimator, RegressorMixin):\n    def __init__(self, estimators):\n        super().__init__()\n        self.estimators = estimators\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def predict(self, X):\n        y_preds = [estimator.predict(X) for estimator in self.estimators]\n        return np.mean(y_preds, axis=0)\n    \n    def predict_proba(self, X):\n        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n        \n        return np.mean(y_preds, axis=0)\n\nmodel = VotingModel(fitted_models_lgb)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:43:22.026105Z","iopub.execute_input":"2024-05-26T16:43:22.026423Z","iopub.status.idle":"2024-05-26T16:43:22.033692Z","shell.execute_reply.started":"2024-05-26T16:43:22.026398Z","shell.execute_reply":"2024-05-26T16:43:22.032699Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Set threshold and correction values\nthreshold = 0.996\ncorrection = 0.05","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:43:22.035045Z","iopub.execute_input":"2024-05-26T16:43:22.035401Z","iopub.status.idle":"2024-05-26T16:43:22.04928Z","shell.execute_reply.started":"2024-05-26T16:43:22.035367Z","shell.execute_reply":"2024-05-26T16:43:22.048316Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(columns=[\"WEEK_NUM\",'target'])\ndf_test = df_test.set_index(\"case_id\")\n\ny_pred = pd.Series(model.predict_proba(df_test)[:,1], index=df_test.index)\ndf_subm = pd.read_csv(\"/kaggle/working/sub.csv\")\ndf_subm = df_subm.set_index(\"case_id\")\n\ndf_subm.loc[y_pred < threshold, 'score'] = (df_subm.loc[y_pred < threshold, 'score'] * threshold - correction).clip(0)\ndf_subm.to_csv(\"submission.csv\")\ndf_subm","metadata":{"execution":{"iopub.status.busy":"2024-05-26T16:43:22.05054Z","iopub.execute_input":"2024-05-26T16:43:22.051093Z","iopub.status.idle":"2024-05-26T16:43:22.102226Z","shell.execute_reply.started":"2024-05-26T16:43:22.051061Z","shell.execute_reply":"2024-05-26T16:43:22.101299Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"            score\ncase_id          \n57543    0.000000\n57549    0.000000\n57551    0.000000\n57552    0.000000\n57569    0.000000\n57630    0.000000\n57631    0.000000\n57632    0.000000\n57633    0.000000\n57634    0.007544","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>case_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57543</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57549</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57551</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57552</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57569</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57630</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57631</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57632</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57633</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>57634</th>\n      <td>0.007544</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}